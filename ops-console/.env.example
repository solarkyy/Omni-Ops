# Optional environment configuration for Ops Console
# Copy to .env and fill in real values if using external APIs

# Server port (default: 3000)
PORT=3000

# Omni-Ops game URL (legacy, for reference)
OMNI_OPS_URL=http://127.0.0.1:8080

# ============================================================================
# GAME API BRIDGE CONFIGURATION
# ============================================================================
# The Ops Console connects to the running game via HTTP API endpoints.
# Set these to match where your game's HTTP bridge server is running.

# Base URL for game API endpoints (default: http://127.0.0.1:8080)
# The bridge will append /game-api/* to this base URL.
# 
# Examples:
#   Local game: http://127.0.0.1:8080
#   Remote game: http://192.168.1.100:8080
#   Custom port:  http://127.0.0.1:9000
OMNI_OPS_GAME_API_BASE=http://127.0.0.1:8080

# Timeout in milliseconds for game API calls (default: 5000)
# Increase if your game is slow to respond, or if running over network.
OMNI_OPS_GAME_API_TIMEOUT=5000

# ============================================================================
# LLM INTEGRATION (AI Provider Configuration)
# ============================================================================
# Configure how Ops Console talks to AI models for Code-AI and Omni-Dev roles.

# AI Provider (default: 'mock')
# Options:
#   - 'mock': Returns mock responses (useful for development without API access)
#   - 'openai': Uses OpenAI API (requires AI_API_KEY with valid sk-... token)
#   - 'local': Calls a local LLM server (e.g., Ollama, LocalAI) at AI_API_BASE
#
# To use real LLM, set to 'openai' or 'local' and configure the corresponding API key/endpoint.
AI_PROVIDER=mock

# Model for Code-AI (senior engineer role)
# Default: 'gpt-4' (for OpenAI provider)
# For local providers, use whatever model name your local server hosts (e.g., 'mistral', 'llama2')
AI_MODEL_CODE=gpt-4

# Model for Omni-Dev AI (in-game AI developer role)
# Default: 'gpt-4' (for OpenAI provider)
# This model receives game snapshot/test context and returns JSON { reply, commands[] }
AI_MODEL_OMNIDEV=gpt-4

# API Key for the provider (if needed)
# For OpenAI: Set to your sk-... API key
# For local provider: Leave blank (no authentication)
# ⚠️  NEVER commit this to version control. Use a .env file instead.
# AI_API_KEY=sk-...

# API Base URL for local LLM providers (default: 'http://localhost:5000')
# Example for Ollama: http://localhost:11434
# Example for LocalAI: http://localhost:8080
# Only used if AI_PROVIDER='local'
# AI_API_BASE=http://localhost:5000

# Timeout in milliseconds for LLM API calls (default: 30000)
# Increase for slower local LLM servers, or if running over network.
# AI_TIMEOUT_MS=30000

# ============================================================================
# DETAILED CONFIGURATION EXAMPLES
# ============================================================================

# Example 1: Use OpenAI GPT-4
# ---
# AI_PROVIDER=openai
# AI_API_KEY=sk-your-actual-key-here
# AI_MODEL_CODE=gpt-4
# AI_MODEL_OMNIDEV=gpt-4
# AI_TIMEOUT_MS=30000

# Example 2: Use local Ollama (running at localhost:11434)
# ---
# AI_PROVIDER=local
# AI_API_BASE=http://localhost:11434
# AI_MODEL_CODE=mistral
# AI_MODEL_OMNIDEV=mistral
# AI_TIMEOUT_MS=60000

# Example 3: Use mock provider for development (no API key needed)
# ---
# AI_PROVIDER=mock
# (all other AI settings are ignored)
